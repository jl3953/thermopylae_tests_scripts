import system_utils

import os
import shlex
import statistics
import subprocess


def build_server(server_node, commit_branch):
    """ Builds the server, given the commit branch.

    Args:
        server_node (dict of Node object)
        commit_branch (str)

    Returns:
        None.
        """
    server_url = server_node["ip"]

    cmd = "cd /root/smdbrpc; " \
          "git fetch origin {0}; " \
          "git checkout {0}; " \
          "git pull origin {0}; ".format(commit_branch)
    print(system_utils.call_remote(server_url, cmd))

    cmd = "cd /root/smdbrpc/cpp/; " \
          "rm -rf cmake/*; " \
          "mkdir -p cmake/build; " \
          "pushd cmake/build; " \
          "export PATH=$PATH:/root/.local/bin; " \
          "cmake -DCMAKE_INSTALL_PREFIX=/root/.local ../..; " \
          "make -j"
    print(system_utils.call_remote(server_url, cmd))


def build_client(client_node, commit_branch):
    """ Builds client.

    Args:
        client_node (dict of Node object)
        commit_branch (str)

    Returns:
        None.
        """
    client_url = client_node["ip"]

    cmd = "cd /root/smdbrpc; " \
          "git fetch origin {0}; " \
          "git checkout {0}; " \
          "git pull origin {0}; ".format(commit_branch)
    print(system_utils.call_remote(client_url, cmd))


def run_server(server_node, concurrency):
    """ Runs server with given concurrency

    Args:
        server_node (dict of Node object)
        concurrency (int)

    Returns:
        process of running server
        """
    server_url = server_node["ip"]

    cmd = "/root/smdbrpc/cpp/cmake/build/hotshard_gateway_async_server {0}" \
        .format(concurrency)
    ssh_wrapped_cmd = "sudo ssh {0} '{1}'".format(server_url, cmd)

    process = subprocess.Popen(shlex.split(ssh_wrapped_cmd))
    return process


def run_clients(client_nodes, server_node, duration, concurrency, batch,
                read_percent, location):
    """ Runs clients.

    Args:
        client_nodes (list of dict of Node objects)
        server_node (dict of Node object)
        duration (int): seconds
        concurrency (int): of clients
        batch (int): number of keys per rpc
        read_percent (int): from 0-100
        location (str): to output raw logs.

    Returns:
        None.
        """
    server_url = server_node["ip"]

    client_processes = []
    i = 0
    logfiles = []
    for client in client_nodes:
        log = os.path.join(location, "raw_data{0}.txt".format(i))
        client_url = client["ip"]
        cmd = "cd /root/smdbrpc/go; " \
              "/usr/local/go/bin/go run hotshard_gateway_client/generate_workload_client.go " \
              "--concurrency {0} " \
              "--batch {1} " \
              "--read_percent {2} " \
              "--host {3} " \
              "--instantaneousStats " \
              "--warmup 1s " \
              "--duration {4}s".format(concurrency,
                                       batch,
                                       read_percent,
                                       server_url,
                                       duration)
        ssh_wrapped_cmd = "sudo ssh {0} '{1}'".format(client_url, cmd)
        with open(log, "w") as f:
            client_processes.append(subprocess.Popen(
                shlex.split(ssh_wrapped_cmd), stdout=f, stderr=f))

        i = i + 1
        logfiles.append(log)

    for cp in client_processes:
        cp.wait()

    return logfiles


def kill(node):
    """ Kills a client or server on given node.

    Args:
        node (dict of Node object)

    Returns:
        None.
        """
    ip = node["ip"]

    cmd = ("PID=$(! pgrep -f cicada) "
           "|| (sudo pkill -9 -f cicada; while ps -p $PID;do sleep 1;done;)")

    system_utils.call_remote(ip, cmd)


def aggregate_raw_logs(logfiles):
    """ Aggregates data from all logfiles generated by run_clients.


    Args:
        logfiles (list[str]): list of logfile locations.

    Returns:
        aggregated data (dict), whether any data was generated (bool)
        """

    def parse_time(time):
        """ Returns time in ms. Expected format: 5ms, 6s, etc."""
        if "ms" in time:
            latency = float(time.strip("ms"))
            return latency
        elif "µs" in time:
            latency = float(time.strip("µs"))
            return latency / 1000
        elif "ns" in time:
            latency = float(time.strip("ns"))
            return latency / (1000 * 1000)
        else:  # seconds
            latency = float(time.strip("s"))
            return latency * 1000

    def parse(line):
        components = line.strip().split('/')
        p99 = parse_time(components[-1].strip())
        p50 = parse_time(components[-2].strip())

        # expected format: "p99 9412.1 qps"
        tp = float(components[-3].split()[1])
        return tp, p50, p99

    total_read_tp = -1
    all_read_p50 = []
    all_read_p99 = []
    total_write_tp = -1
    all_write_p50 = []
    all_write_p99 = []
    total_tp = -1
    for log in logfiles:

        with open(log, "r") as f:
            for line in f:
                try:
                    if "Read throughput" in line:
                        read_tp, read_p50, read_p99 = parse(line)
                        total_read_tp += read_tp
                        total_tp += read_tp
                        all_read_p50.append(read_p50)
                        all_read_p99.append(read_p99)
                    elif "Write throughput" in line:
                        write_tp, write_p50, write_p99 = parse(line)
                        total_write_tp += write_tp
                        total_tp += write_tp
                        all_write_p50.append(write_p50)
                        all_write_p99.append(write_p99)
                    else:
                        continue
                except BaseException as e:
                    continue

    # calculate read p50
    read_p50 = statistics.mean(all_read_p50)
    read_p99 = statistics.mean(all_read_p99)
    write_p50 = statistics.mean(all_write_p50)
    write_p99 = statistics.mean(all_write_p99)
    p50 = statistics.mean(all_read_p50 + all_write_p50)
    p99 = statistics.mean(all_read_p99 + all_read_p99)

    result = {
        "ops/sec(cum)": total_tp, # DON'T CHANGE THIS KEY
        "p50(ms)": p50, # OR THIS ONE
        "p99(ms)": p99, # OR THIS ONE
        "read_tp": total_read_tp,
        "read_p50": read_p50,
        "read_p99": read_p99,
        "write_tp": total_write_tp,
        "write_p50": write_p50,
        "write_p99": write_p99,
    }
    return result, True


def parse_raw_logfiles(input_logfiles, output_csvfile):
    return "tee hee"


def graph(datafile, graph_location):
    return "more tee hee"
